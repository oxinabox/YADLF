#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
YADLF
\end_layout

\begin_layout Subtitle
Year Another Deep Learning Framework
\end_layout

\begin_layout Author
Lyndon White
\begin_inset Newline newline
\end_inset

University of Western Australia,
\begin_inset Newline newline
\end_inset

 School of Electrical Electronic and Computer Engineering
\begin_inset Newline newline
\end_inset

Signals and Information Processing Lab
\end_layout

\begin_layout Date
\begin_inset Phantom Phantom
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
This text originally appears as an appendix to my honours paper: 
\begin_inset Quotes eld
\end_inset

DNN Low Level Reinitialization: A Method for Enhancing Learning in Deep
 Neural Networks through Knowledge Transfer
\begin_inset Quotes erd
\end_inset

 (Lyndon White, October 2014).
\end_layout

\begin_layout Standard
Large portions of the text originally appears in the Project Proposal for
 
\begin_inset Quotes eld
\end_inset

Domain Adaptation in Deep Belief Networks
\begin_inset Quotes erd
\end_inset

 (Lyndon White, May 2014).
\end_layout

\begin_layout Section
The YADLF Framework
\end_layout

\begin_layout Standard
A framework for neural network based machine learning has been created.
 The library created is titled, semi-ironically, YADLF or Yet Another Deep
 Learning Framework.
 At the conclusion of this project YADLF will be released as a open source
 project to allow it to be used by others in their research, or as a learning
 resource for future implementers.
\end_layout

\begin_layout Standard
Existing frameworks such as GPUMLib and Pylearn2 were examined and considered
 unsuitable.
 In particular, it was found that frameworks were typically purpose-constructed
 by the research authors.
 Existing frameworks being too inflexible to allow new techniques to be
 easily explored.
\end_layout

\begin_layout Standard
Further, as marginalizing stacked denoising autoencoders
\begin_inset CommandInset citation
LatexCommand cite
key "chen2014marginalized"

\end_inset

 (mSDA) are a fairly new they have not been incorporated into any existing
 larger deep learning library.
 
\end_layout

\begin_layout Standard
Constructing this framework has been a positive educational exercise, and
 has lead to a better understanding of the algorithms involved.
 With improved understanding of how it all works together, it was easier
 to implement adjustments required which has allowed for this project to
 accomplish its goals.
 A variety of technologies have been used to create YADLF.
\end_layout

\begin_layout Standard
The framework was created in Python 2.7
\begin_inset CommandInset citation
LatexCommand cite
key "van2010python"

\end_inset

 using a number of tools, as were found to be common in research papers.
 Preeminently amongst the tools used are numpy
\begin_inset CommandInset citation
LatexCommand cite
key "pythonforscience"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Scipy"

\end_inset

, Cython
\begin_inset CommandInset citation
LatexCommand cite
key "behnel2010cython"

\end_inset

 and IPython
\begin_inset CommandInset citation
LatexCommand cite
key ":/content/aip/journal/cise/9/3/10.1109/MCSE.2007.53"

\end_inset

.
 Numpy is a library for working with the SIMD (Single Instruction Multiple
 Data) components of the CPU
\begin_inset CommandInset citation
LatexCommand cite
key "pythonforscience"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Scipy"

\end_inset

.
 Using numpy is very similar in techniques and speed to using Matlab.
 Cython is a extension to the Python language that compiles Python via C
 into native machine code
\begin_inset CommandInset citation
LatexCommand cite
key "behnel2010cython"

\end_inset

.
 Through profiling the most critical sections where Cython gives speed improveme
nt have been determined, and rewritten using Cython.
 Using these tools a well featured and flexible machine learning library
 was implemented.
\end_layout

\begin_layout Subsection
Features
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "par:Features"

\end_inset


\end_layout

\begin_layout Standard
The framework has the following features.
\end_layout

\begin_layout Itemize
Dataset techniques:
\end_layout

\begin_deeper
\begin_layout Itemize
Standardized
\begin_inset CommandInset citation
LatexCommand cite
key "lecun2012efficient"

\end_inset

 
\begin_inset Foot
status open

\begin_layout Plain Layout
Implemented in current version using scikit-learn preprocessing library
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-learn"

\end_inset

, for speed.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Balancing of order of training case classes
\begin_inset CommandInset citation
LatexCommand cite
key "lecun2012efficient"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010pracrbm"

\end_inset

, while preserving separation of test and training data.
\end_layout

\begin_layout Itemize
Serialization for fast loading
\begin_inset CommandInset citation
LatexCommand cite
key "bilina2012python"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
RBMs:
\end_layout

\begin_deeper
\begin_layout Itemize
Bernoulli-Bernoulli RBMs (BB-RBM)
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2009learningch5"

\end_inset

 
\end_layout

\begin_layout Itemize
Gaussian-Bernoulli RBMs (GB-RBM)
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset

 -- with standardized input only.
 
\end_layout

\begin_layout Itemize
Contrastive Divergence
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2002contrastive_divergance"

\end_inset

 training algorithm, with momentum
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010pracrbm"

\end_inset

, mini-batch learning
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010pracrbm"

\end_inset

 and L2 Weight Decay 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2010pracrbm"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Feed Forward Neural Networks:
\end_layout

\begin_deeper
\begin_layout Itemize
Back-propagation
\begin_inset CommandInset citation
LatexCommand cite
key "rumelhart1986learning"

\end_inset

 with momentum
\begin_inset CommandInset citation
LatexCommand cite
key "rumelhart1986learning"

\end_inset

, mini-batch learning
\begin_inset CommandInset citation
LatexCommand cite
key "lecun2012efficient"

\end_inset

 and L2 Weight Decay 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton1993keeping"

\end_inset


\end_layout

\begin_layout Itemize
Early Stopping
\begin_inset CommandInset citation
LatexCommand cite
key "prechelt1998early"

\end_inset

, with optional learning rate adaption
\end_layout

\begin_layout Itemize
Variety of neuron activation functions including softmax
\begin_inset CommandInset citation
LatexCommand cite
key "Bridle1990"

\end_inset

 and sigmoid
\begin_inset CommandInset citation
LatexCommand cite
key "rumelhart1986learning"

\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
DBNs
\end_layout

\begin_deeper
\begin_layout Itemize
Greedy Layer-wise Training 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2009learningch6"

\end_inset


\end_layout

\begin_layout Itemize
Conversion to initialized DNN
\begin_inset CommandInset citation
LatexCommand cite
key "GoeffHinton.3052013."

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
mSDAs
\end_layout

\begin_deeper
\begin_layout Itemize
mSDA full batch training
\begin_inset CommandInset citation
LatexCommand cite
key "chen2014marginalized"

\end_inset


\end_layout

\begin_layout Itemize
Conversion to initialized DNN
\begin_inset CommandInset citation
LatexCommand cite
key "GoeffHinton.3052013."

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Knowledge Matrix Restructuring Techniques
\end_layout

\begin_deeper
\begin_layout Itemize
As the focus of this project was investigation into transfer of knowledge
 YADLF, unlike any existent deep learning library, comes with substantial
 functionality for manipulating the knowledge of neural architectures.
 Those being the bias and weight matrices.
 It does this without destroying any knowledge contained within the layers.
\end_layout

\begin_layout Itemize
Reinitialization of layers (Used in DNN LLR to reset the bottom layer)
\end_layout

\begin_layout Itemize
Widening of layers, making them wider with new neurons.
\end_layout

\begin_layout Itemize
Adding layers to the top of the network
\end_layout

\end_deeper
\begin_layout Standard
YADLF works well with the Data Analysis tools discussed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Results-Analysis-Tools"

\end_inset

.
\end_layout

\begin_layout Section
Data Analysis and Presentation Tools
\begin_inset CommandInset label
LatexCommand label
name "sec:Results-Analysis-Tools"

\end_inset


\end_layout

\begin_layout Standard
Analysis of results and production of plots for this report was carried
 out using well developed scientific Python ecosystem.
 
\end_layout

\begin_layout Itemize
IPython is a Python interface, which resembles the Mathematica notebook
\begin_inset CommandInset citation
LatexCommand cite
key ":/content/aip/journal/cise/9/3/10.1109/MCSE.2007.53"

\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
It was used as an interface for the majority of interaction with the data
 analysis tools.
\end_layout

\begin_layout Itemize
It also supports process dispatching across multiple cores.
 This was used in preliminary analysis
\end_layout

\end_deeper
\begin_layout Itemize
Pandas 
\begin_inset CommandInset citation
LatexCommand cite
key "mckinney-pandas"

\end_inset

 what used for data analysis
\end_layout

\begin_deeper
\begin_layout Itemize
Tasks such as finding the means and standard deviations, on a basis of per
 topology and training set size, are findable in a few lines of code.
\end_layout

\end_deeper
\begin_layout Itemize
Matplotlib
\begin_inset CommandInset citation
LatexCommand cite
key "Hunter:2007"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Matplotlib is the standard underlying plotting and graphing framework for
 python.
\end_layout

\begin_layout Itemize
It is used by Pandas and Seaborn.
\end_layout

\end_deeper
\begin_layout Itemize
Seaborn
\begin_inset CommandInset citation
LatexCommand cite
key "seaborn"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Seaborn is a data visualization tool
\end_layout

\begin_layout Itemize
It draws on the capacities of matplotlib and Pandas to allow many more types
 of plots and charts to be created
\end_layout

\end_deeper
\begin_layout Itemize
The TeX package Tikz
\begin_inset CommandInset citation
LatexCommand cite
key "tikz"

\end_inset

 was used to generate the diagrams used thought this work.
\end_layout

\begin_layout Itemize
This paper was was created in LyX
\begin_inset CommandInset citation
LatexCommand cite
key "lyx"

\end_inset

, and typeset using LuaTeX
\begin_inset CommandInset citation
LatexCommand cite
key "luatex"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
